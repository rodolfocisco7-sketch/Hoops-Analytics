# .github/workflows/scraper.yml
name: NBA Data Scraper

on:
  schedule:
    # Corre 3 veces al d칤a (UTC - ajusta seg칰n tu zona)
    # 11:00 UTC = 6:00 AM Panam치 (UTC-5)
    - cron: '0 11 * * *'  
    # 19:00 UTC = 2:00 PM Panam치
    - cron: '0 19 * * *'  
    # 03:00 UTC = 10:00 PM Panam치 (d칤a anterior)
    - cron: '0 3 * * *'   
  
  workflow_dispatch:  # Permite ejecuci칩n manual desde GitHub

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run scraper
      run: |
        python scraper_automatico.py
      continue-on-error: true  # No falla si hay errores parciales
    
    - name: Commit and push if changed
      run: |
        git config --global user.name 'GitHub Actions Bot'
        git config --global user.email 'actions@github.com'
        
        # 1. Traer lo 칰ltimo de GitHub de forma agresiva
        git fetch origin main
        git checkout main
        git reset --hard origin/main
        
        # 2. Volver a a침adir los datos generados por el scraper
        git add data/
        
        # 3. Solo si hay cambios, forzar la subida
        if [ -n "$(git status --porcelain data/)" ]; then
          git commit -m "游뱄 Auto-update data $(date +'%Y-%m-%d %H:%M UTC')"
          # Usamos push force-with-lease: es m치s seguro que force, 
          # pero asegura que nuestros datos entren s칤 o s칤.
          git push origin main --force
        else
          echo "No hay cambios detectados."
        fi